---
title: "NYC Green Taxi Tips"
output: html_document
date: "2025-04-08"
---

For this project, we will be using the  dataset **NYC Green Taxis from 2018**. Our main objective is to evaluate the factors that contribute toward cab drivers being incentivized (i.e what determines whether or not they receive a tip).


#### CRISP-DM: Data Understanding 

```{r}
# Load libraries
library(tidyverse)
library(lubridate)
library(openintro)
library(caret)
library(class)
library(FNN)
library(psych)
library(ggcorrplot)
```

#### Load data

Load the NYC Green Taxi Trip Records data into a dataframe

```{r}
#Read CSV data and load into a dataframe
tripdata_df <- read.csv("/Users/hetvichudasama/Downloads/2018_Green_Taxi_Trip_Data-1.csv")
```

#### Data exploration 

#### The distribution of data 

Explore the data to identify any patterns and analyze the relationships between the features and the target variable (tip amount)

```{r}
# Inspect the dimensions of the dataset
dim(tripdata_df)
```

The 2018 Green Taxi Trip Records data has 1048575 rows and 19 columns. Each observation is recorded in a row. 

```{r}
# Get a glimpse of the dataset structure
str(tripdata_df)
```

```{r}
# Summary statistics and missing values
summary(tripdata_df)
```

There are 3 missing values on column "trip_type". However, the entire column "ehail_fee" has NA values because the number of NA's equals the number of rows in the dataframe (1048575).

Five columns that are factors are `payment_type`, `RateCodeID`, `trip_type`, `VendorID`, `store_and_fwd_flag`.

```{r}
# Convert relevant columns to factors
tripdata_df$payment_type <- as.factor(tripdata_df$payment_type)
tripdata_df$RatecodeID <- as.factor(tripdata_df$RatecodeID)
tripdata_df$trip_type <- as.factor(tripdata_df$trip_type)
tripdata_df$VendorID <- as.factor(tripdata_df$VendorID)
tripdata_df$store_and_fwd_flag <- as.factor(tripdata_df$store_and_fwd_flag)

# Check structure again
str(tripdata_df)
```

The `lpep_pickup_datetime` and the `lpep_dropoff_datetime` columns need to be properly formatted as Date POSIXct, as they are currently in 'character' type.

```{r}
# Convert the pick up and drop off date and time to a Date object
tripdata_df$lpep_pickup_datetime <- as.POSIXct(tripdata_df$lpep_pickup_datetime, format="%m/%d/%Y %H:%M")
tripdata_df$lpep_dropoff_datetime <- as.POSIXct(tripdata_df$lpep_dropoff_datetime, format="%m/%d/%Y %H:%M")
```

The `fare_amount` column is the time-and-distance fare calculated by the meter. This column should be in numeric type instead of character type.
The `total_amount` column is The total amount charged to passengers and does not include cash tips. This column should be in numeric type instead of character type. 

```{r}
# Convert other columns to numeric type 
tripdata_df$fare_amount <- as.numeric(tripdata_df$fare_amount)
tripdata_df$total_amount <- as.numeric(tripdata_df$total_amount)
tripdata_df$PULocationID <- as.numeric(tripdata_df$PULocationID)
tripdata_df$DOLocationID <- as.numeric(tripdata_df$DOLocationID)
tripdata_df$passenger_count <- as.numeric(tripdata_df$passenger_count)
```

```{r}
# Check if the pickup time is later than or equal to the drop-off time
inconsistent_data <- tripdata_df %>%
  filter(tripdata_df$lpep_pickup_datetime >= tripdata_df$lpep_dropoff_datetime)

nrow(inconsistent_data)

head(inconsistent_data, 5)
```

There are 10135 rows of inconsistent data where the drop off time is the same as the pick up time.

```{r}
# Check the summary statistics again
summary(tripdata_df)
```

Column `RateCodeID` is the final rate code in effect at the end of the trip, ranging from 1-6. Maximum value shown here is 99, which means there must be some outliers in this column.

Column `Extra`, `mta_tax`, `tip_amount`, `improvement_surcharge` have negative values for minimum (-4.5, -0.5, -2.72, -0.3). These values seems not valid because these charges cannot be negative. There must be some outliers in these columns too.

Column `ehail_fee` is a `logi` (logical) variable, but the column appears to contain mostly missing values NA. This column will not add meaningful insights to the analysis. 

Column `fare_amount` is the time-and-distance fare calculated by the meter. The values should not be negative but it has negative min value as -183. The column has 5 missing values.

Column `total_amount` is the total amount charged to passengers. The values should not be negative. However, looking at some data, it has negative value like -183. The column has 5 missing values.

We want to visualize the distributions in `trip_distance` using a histogram

```{r}
# Visualize the distribution of trip_distance 
ggplot(tripdata_df, aes(x = trip_distance)) +
  geom_histogram(binwidth = 3, fill = "steelblue", color = "black") +
  labs(title = "Trip Distance (miles)",
       subtitle = "Frequency of trip distances",
       x = "Distribution of Trip Distance", 
       y = "Count",
       caption = "See the distribution of Trip Distance") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The data is positively skewed because most data clusters on the left and tail extends to the right. Most outliers are likely to appear on the right side of the distribution. We can confirm this by calculating mean and median values of trip distance. 

We want to visualize the distributions of `tip_amount` using a histogram

```{r}
# Visualize the distribution of tip_amount 
ggplot(tripdata_df, aes(x = tip_amount)) +
  geom_histogram(binwidth = 3, fill = "steelblue", color = "black") +
  labs(title = "Tip Amount (USD)",
       subtitle = "Frequency of tip amount",
       x = "Distribution of Tip Amount", 
       y = "Count",
       caption = "See the distribution of Tip Amount") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The distribution of tip_amount is right-skewed with a long tail to the right side of the plot. This means most of the values are low. Some high tip_amount values to the right are outliers. This helps us understand the tipping behavior of passengers. 

We want to visualize the distributions of `fare_amount` using a histogram

```{r}
# Visualize the distribution of fare_amount
ggplot(tripdata_df, aes(x = fare_amount)) +
  geom_histogram(binwidth = 30, fill = "steelblue", color = "black") +
  labs(title = "Fare Amount (USD)", 
       subtitle = "Frequency of fare amount",
       x = "Distribution of Fare Amount", 
       y = "Count",
       caption = "See the distribution of Fare Amount") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The distribution of fare_amount is right-skewed with a long tail to the right side. Most fare amounts are low (around 0 column). There are some fare amounts that exceed $200. Some even have negative values. These negative values might be outliers or data entry error that we need to handle later. 

We want to visualize the distributions of `passenger_count` using a histogram

```{r}
# Visualize the distribution of passenger_count
ggplot(tripdata_df, aes(x = passenger_count)) +
  geom_bar(stat = "count", fill = "steelblue") +
  labs(title = "Passenger Count", 
       subtitle = "Frequency of passenger count",
       x = "Distribution of Passenger Count", 
       y = "Count",
       caption = "See the distribution of Passenger Count") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The histogram above shows the distribution of passenger count. Passenger count often ranges from 0 to 9. The most frequent number of passengers is one. After 6 passengers count, the columns drop significantly. The high frequency of 0 seems to be outliers because the taxi drive cannot have 0 passengers.

We want to visualize the distributions of `extra` using a histogram

```{r}
# Visualize the distribution of extra charges 
ggplot(tripdata_df, aes(x = extra)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Extra and Surcharges (USD)", 
       x = "Distribution of extra and surcharges", 
       y = "Frequency",
       subtitle = "Frequency of extra and surcharges applied",
       caption = "See the distribution of extra and surcharges") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The distribution of extra and surcharges show that the majority of the extra charges are low. According to the data dictionary, it says the charges only include 0.5 and 1 for rush hour and overnight charges. Here, we can see some charges more than 2.5 and negative charges. These values might be outliers that we need to handle later. 

We want to visualize the distributions of `MTA Tax` using a histogram

```{r}
# Visualize the distribution of MTA_tax
ggplot(tripdata_df, aes(x = mta_tax)) +
  geom_bar(stat = "count", fill = "steelblue") +
  labs(title = "MTA Tax (USD)", 
       x = "Distribution of extra and surcharges", 
       y = "Frequency",
       subtitle = "Frequency of MTA Tax",
       caption = "See the distribution of MTA Tax") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

Most common MTA tax values are around 0.5. The second peak is 0, which seems to be wrong because according to the data dictionary, 0.5 MTA tax is automatically triggered based on the rate in use. The data should not have 0 values in there. Also, there are some negative values that need to be addressed here. 

We want to visualize the distributions of `tolls_amount` using a histogram

```{r}
# Visualize the distribution of tolls amount
ggplot(tripdata_df, aes(x = tolls_amount)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Tolls Amount (USD)", 
       x = "Distribution of tolls amount", 
       y = "Frequency",
       subtitle = "Frequency of tolls amount",
       caption = "See the distribution of tolls amount") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The histogram above shows the frequency of tolls amount. The distribution seems to be right-skewed. Most common data is 0, meaning that most trips do not have tolls charges. Some trips have extremely high tolls charges, surpassing 400 dollars but these are much less frequent. These values can also be outliers that need to handled later. 

We want to visualize the distributions of `total_amount` using a histogram

```{r}
# Visualize the distribution of total_amount
ggplot(tripdata_df, aes(x = total_amount)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Total Amount (USD)", 
       x = "Distribution of total amount", 
       y = "Frequency",
       subtitle = "Frequency of total amount",
       caption = "See the distribution of total amount") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The distribution seems to be right-skewed with a lot of values are frequently to be close to 0. Some high amounts on the right tail can be outliers. We also see negative values here which might be outliers or data entry errors too. Overall, this histogram helps us understand the total amount which does not include cash tips paid by passengers.

We want to visualize the distributions of `Improvement_surcharge ` using a histogram

```{r}
# Visualize the distribution of improvement surcharges
ggplot(tripdata_df, aes(x = improvement_surcharge)) +
  geom_bar(stat = "count", fill = "steelblue") +
  labs(title = "Improvement Surcharges (USD)", 
       x = "Distribution of improvement surcharges", 
       y = "Frequency",
       subtitle = "Frequency of improvement surcharges",
       caption = "See the distribution of improvement surcharges") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The histogram shows the most common improvement surcharges are 0.3. There are some negative values which should not be in this column and will be handled later. 

Our dependent variable is `tip_amount`. Let's visualize the distributions of `tip_amount ` using a histogram

```{r}
# Visualize the distribution of tip amount
ggplot(tripdata_df, aes(x = tip_amount)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Tip Amount (USD)", 
       x = "Distribution of tip amount", 
       y = "Frequency",
       subtitle = "Frequency of tip amount",
       caption = "See the distribution of tip amount") + 
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The distribution of tip amount is right-skewed with a long tail to the right side. This column only has tip amount that is populated for credit card tips. Cash tips are not included. There is a large frequency of 0 values in the histogram. 

We can also visualize the distribution of `Payment Type` using a bar plot
```{r}
# Visualize the distribution of payment_type
ggplot(tripdata_df, aes(x = payment_type)) +
  geom_bar(stat = "count", fill = "steelblue") +
  labs(title = "Payment Type",
       subtitle = "Frequency of payment type",
       x = "Distribution of Payment type",
       y = "Count",
       caption = "See the distribution of Payment Type") +
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))

```

Credit Card is the most prevalent method of payment. Passengers have a strong preference for credit card payments. 

#### The correlations

Next, we created some visualizations to see the correlations between the tip amount and some categorical variables 

```{r}
# Visualize the distribution of payment_type across RatecodeID
ggplot(tripdata_df, aes(x = RatecodeID, y = tip_amount)) +
  geom_boxplot() +
  labs(title = "The correlation between RatecodeID and Tip Amount ",
       subtitle = "How tip amounts are different across rate codes",
       x = "Rate Code ID",
       y = "Tip Amount (USD)",
       caption = "The boxplot shows the distribution of tip amounts for different rate codes") +
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))

```

The boxplot shows the distribution of tip amount across different Ratecode ID. There are 6 rate codes ranging from 1 to 6. However, we can see category 99 on the plot, which is the outlier that should be handled later. Categories 1 and 5 show similar median tip amounts, both clustered around a relatively lower value compared to the others. In contrast, categories 2, 3, and 4 exhibit higher median tips, indicating that trips with these rate codes may receive more generous tipping. While all categories show the presence of outliers, category 1 appears to have a larger number of extreme outliers.

```{r}
# Visualize the distribution of payment_type across store_and_fwd_flag
ggplot(tripdata_df, aes(x = store_and_fwd_flag, y = tip_amount)) +
  geom_boxplot() +
  labs(title = "The correlation between RatecodeID and Tip Amount ",
       subtitle = "How tip amounts are different across store and forward flag",
       x = "Store and Forward Flag",
       y = "Tip Amount (USD)",
       caption = "The boxplot shows the distribution of tip amounts based on store and forward flag") +
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The boxplot above illustrates the distribution of “tip_amount” across the two categories of the “store_and_fwd_flag” variable: ‘N’ (not a store and forward trip) and ‘Y’ (store and forward trip). This visualization helps clarify how tip amounts relate to whether the store-and-forward feature was used. Both categories have similar median tip amounts, each centered around a relatively low value. However, the ‘N’ category displays a greater number of extreme outliers than the ‘Y’ category. To ensure accurate analysis, these outliers should be addressed appropriately.

```{r}
# Visualize the distribution of payment_type across payment type
ggplot(tripdata_df, aes(x = payment_type, y = tip_amount)) +
  geom_boxplot() +
  labs(title = "The correlation between Payment Type and Tip Amount ",
       subtitle = "How tip amounts are different across payment type",
       x = "Payment Type",
       y = "Tip Amount (USD)",
       caption = "The boxplot shows the distribution of tip amounts based on payment type") +
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The boxplot illustrates how “tip_amount” is distributed across various “payment_type” categories, offering insights into how tipping differs by payment method. The plot includes five payment types: 1, 2, 3, 4, and 5. The median tip amounts differ among these categories, with types 3 and 4 showing relatively higher medians, and type 2 maintaining a consistently elevated median. Payment type 1 exhibits the widest range of tip amounts and contains the most extreme outliers, indicating significant variability in tips for this method. Although types 3 and 4 also contain outliers, they are less prominent compared to those in type 1. This suggests that payment type 1 is associated with more inconsistent tipping behavior.

```{r}
# Visualize the distribution of payment_type across trip type
ggplot(tripdata_df, aes(x = trip_type, y = tip_amount)) +
  geom_boxplot() +
  labs(title = "The correlation between Trip Type and Tip Amount ",
       subtitle = "How tip amounts are different across trip type",
       x = "Trip Type",
       y = "Tip Amount (USD)",
       caption = "The boxplot shows the distribution of tip amounts based on trip type") +
  scale_y_log10() +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The boxplot presents the distribution of “tip_amount” across different “trip_type” categories, offering insights into how tipping patterns vary by trip type. The data includes two main trip types—1 and 2—as well as some NA values. The median tip amounts for both trip types are quite similar, each centered around a relatively low value. While both categories contain outliers, trip type 1 shows more extreme cases. These outliers reflect variability in tipping behavior across trip types and should be carefully handled during the data cleaning process to ensure reliable analysis.

Then, we created a linear regression model to check the correlato
```{r}
# Check all variables if they are numeric
tripdata_numeric <- tripdata_df[sapply(tripdata_df, is.numeric)]

# Build a full correlation matrix 
cor_matrix <- cor(tripdata_numeric, use="complete.obs")

cor_matrix

# Visualize the cor_matrix
library(ggcorrplot)
ggcorrplot(cor_matrix, lab = TRUE)
```
```{r}
set.seed(123) # for reproducibility

# Sample 1000 random rows to see the patterns 
sample_data <- sample_n(tripdata_numeric, 1000)

# See the correlations between all pairs of variables.
library(psych)
pairs.panels(sample_data)
```

`trip_distance`, `fare_amount`, `tip_amount`, `total_amount` plots all show right-skewed distributions with a long tail to the right side. Some variables like extra and mta_tax have discrete values, visible as distinct bars in their histograms. Scatter plots reveal the presence of outliers, especially in trip_distance and fare_amount.

The strongest positive correlations with tip_amount are `fare_amount` (0.35) and `trip_distance` (0.31). `passenger_count`, `extra`, and `mta_tax` show very weak correlations with `tip_amount` (all below 0.05).

Multicollinearity occurs when the independent variables show moderate to high correlation. In a model with correlated variables, it is difficult to figure out the true relationship between predictors and the dependent variable. When correlated predictors are present, the standard errors tend to increase. Therefore, we would make an assumption that the data has no multicollinearity, meaning the independent variables are not highly correlated with each other.

`fare_amount` and `total_amount`: Highly correlated (0.977). total_amount includes fare, tax, and tip (not cash tips), so it causes data leakage. fare_amount should be used, and total_amount should be excluded.

`mta_tax` and `improvement_surcharge`: Highly correlated (0.983). Both are components of the fare. It's better to exclude these fixed surcharge components and let the model learn the base fare relationship.

`trip_distance` and `fare_amount`: Highly correlated (0.899), which is expected since fare is largely based on distance.

`trip_distance` and `total_amount`: Highly correlated (0.891), which is expected since total fare is also based on distance.


#### Missing values 

To check for missing values, we counted all the missing values in each column 

```{r}
# Check missing values 
missing_values <- colSums(is.na(tripdata_df))

print(missing_values) 
```

The data has 5 missing values in the `fare_amount`, 5 missing values in the `total_amount`, 3 missing values in the `trip_type` and 1048575 missing values in the `ehail_fee` columns. The entire ehail_fee column has missing values. These missing values will be handled when we preprocess the data. 

#### Outliers analysis

To analyze the outliers for `Trip Distance`, we will inspect values that lie more than 3 standard deviations away from the mean. 

```{r}
# Calculate key descriptive statistics
mean_trip_distance <- mean(tripdata_df$trip_distance)
sd_trip_distance <- sd(tripdata_df$trip_distance)

# Calculate the z-score 
tripdata_df$z2 <- abs((mean_trip_distance - tripdata_df$trip_distance) / sd_trip_distance)
tripdistance_outliers <- which(tripdata_df$z2 > 3)

dim(tripdata_df[tripdistance_outliers,])

head(tripdata_df[tripdistance_outliers,],5)
```

There are 20953 outliers in the trip_distance variable. The first 5 outliers are presented above. 

To analyze the outliers for the `tip_amount`, we will inspect values that lie more than 3 standard deviations away from the mean

```{r}
# Calculate key descriptive statistics
mean_tip_amount <- mean(tripdata_df$tip_amount)
sd_tip_amount <- sd(tripdata_df$tip_amount)

# Calculate the z-score 
tripdata_df$z3 <- abs((mean_tip_amount - tripdata_df$tip_amount) / sd_tip_amount)
tipAmount_outliers <- which(tripdata_df$z3 > 3)

dim(tripdata_df[tipAmount_outliers,])

head(tripdata_df[tipAmount_outliers,],5)
```

There are 14444 outliers in the tip_amount variable. The first 5 outliers are presented above. 

#### Feature selection

We tried to build a full multiple regression model that predicts the tip amount

```{r}
# Construct the model
model <- lm(tip_amount ~., data = tripdata_numeric)
summary(model)
```

Based on the linear regression model above, all predictor variables have p-values less than 0.05, indicating that they are statistically significant. This suggests that each variable has a meaningful contribution to the model. Additionally, the Multiple R-squared value is 0.985, which is very high. This indicates that the model explains approximately 98.5% of the variance in the tip amount, demonstrating strong predictive power. Overall, the low p-values and high R-squared value suggest that the relationship between the predictors and the tip amount is unlikely to be due to random chance.

However, when we consider which variable should be included in the model, we also need to think about multicollinearity. Based on all these results, we initially made this list of useful variables and not-so-useful variables.

***Predictor variables that might be useful for tip_amount:***

1. fare_amount: The base fare on which the tip is calculated. Although it has a strong correlation with total_amount which suggests multicollinearity, we will just keep fare_amount but not total_amount in our model.  

2. total_amount: it has strong positive correlation with tip_amount (0.4728). It also has a high coefficient in the linear regression model (9.694e-01), indicating its importance in predicting tip_amount.

3. trip_distance: Influences the fare and, thus, the tip. Longer trips may result in higher tips. It has moderate positive correlation with tip_amount (0.3151). Positive coefficient (0.002339) in the regression model, showing its contribution to tip prediction.

4. tolls_amount: Higher tolls might influence tip percentage. It has moderate correlation with tip_amount (0.1389)

5. extra: Extra charges such as rush hour or overnight charges may influence tipping behavior. 

***Variables that might be excluded from the analysis***

1. mta_tax and improvement_surcharge: They have high multicollinearity and relatively fixed values.

2. passenger_count: Extremely low correlation with tip_amount (0.0029).

3. PULocationID and DOLocationID: Although it might matter if specific locations are high-tipping zones, but not a general predictor.

We tested the model after excluding these variables to see if the model's prediction power has been improved.

Because `fare_amount` and `total_amount` has multicollinearity, we tested the model to see if one of these two could be excluded from the analysis. 

```{r}
# Removel fare_amount from model
model_1 <- update(model, . ~ . - fare_amount)

summary(model_1)
```

After removing `fare_amount` from the model (model_1), the Multiple R-squared dropped significantly from 0.985 to 0.2811. This substantial decrease indicates that `fare_amount` was a key predictor in explaining the variability in `tip_amount`. Its exclusion greatly reduced the model's ability to predict tip amounts accurately.

```{r}
# Remove total_amount from model
model_2 <- update(model, . ~ . - total_amount)

summary(model_2)
```

After removing `total_amount` from the model (model_2), the Multiple R-squared dropped dramatically from 0.985 to 0.1108. This sharp decrease indicates that `total_amount` was a crucial predictor for explaining the variability in `tip_amount.` Its removal significantly weakened the model’s predictive performance. 

If we remove all the variables that we put in our hypothesis, we 

```{r}
# Remove variables from model
model_3 <- update(model, . ~ . - mta_tax - improvement_surcharge - passenger_count - PULocationID - DOLocationID)

summary(model_3)
```

After removing `mta_tax`, `improvement_surcharge`, `passenger_count`, `PULocationID`, and `DOLocationID` from the model (model_3), the Multiple R-squared value remains high at 0.9803. This indicates that the removed variables contribute relatively little to the overall explanatory power of the model. The predictors that remain—especially total_amount—still account for most of the variability in tip_amount. Therefore, these excluded variables may not be essential for the model’s predictive performance and could be considered for removal to simplify the model without substantially compromising its accuracy.

The p-value for trip_distance is 0.122 > 0.05. We might consider to exclude the trip_distance from the model. 

```{r}
# Remove trip_distance + other variables from model
final_model<- update(model, . ~ . - mta_tax - improvement_surcharge - passenger_count - PULocationID - DOLocationID - trip_distance)

summary(final_model)
```

In the final model, after removing `mta_tax`, `improvement_surcharge`, `passenger_count`, `PULocationID`, `DOLocationID`, and `trip_distance`, the Multiple R-squared remains high at 0.9803, indicating that the model still explains a very large proportion of the variability in tip_amount. This suggests that the four remaining predictors—fare_amount, extra, tolls_amount, and especially total_amount—are the key variables driving the model’s predictive power.

All coefficients are highly statistically significant (p < 2e-16), and the residual standard error is low (0.263), indicating good model fit. 

**Useful variables**: fare_amount, extra, tolls_amount, total_amount

**Excluding variables**: mta_tax, improvement_surcharge, passenger_count, PULocationID, DOLocationID, trip_distance

#### Feature engineering

We will include a column called `trip_duration` because we think trip duration could be a useful predictor of tip amounts. Longer trips might lead to higher tips, as passengers may perceive extended service or effort from the driver.

```{r}
# Create trip_duration column in minutes
tripdata_df$trip_duration <- as.numeric(difftime(tripdata_df$lpep_dropoff_datetime, 
                                                 tripdata_df$lpep_pickup_datetime, 
                                                 units = "mins"))

# Check summary statistics again
summary(tripdata_df)
```

The median of the trip_duration column is 10 minutes while the max value is 3383 minutes (approximately 56 hours). There has to be outliers in this column. The difference between the mean (19.99 minutes) and the median (10 minutes) indicates that the distribution is right-skewed. This means that there are a number of trips with much longer durations pulling the average up.

```{r}
# Calculate correlation coefficient 
cor(tripdata_df$trip_duration, tripdata_df$tip_amount, use = "complete.obs")

# Visualization of relationship between trip_duration and tip_amount
ggplot(tripdata_df, aes(x = trip_duration, y = tip_amount)) +
  geom_point(alpha = 0.3, color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Scatter Plot of Trip Duration vs. Tip Amount",
       x = "Trip Duration (minutes)",
       y = "Tip Amount",
       caption = "Check if there is a linear relationship between Trip Duration and Tip Amount ") +
  theme_minimal() +
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

The Pearson correlation coefficient between Trip Duration and Tip Amount is 0.01191982. It indicates a very weak positive linear relationship. This means that as the trip duration increases, there is only a minimal tendency for the tip amount to increase. The relationship is so weak that, for practical purposes, trip_duration and tip_amount can be considered uncorrelated. The scatter plot also supports this conclusion. Therefore, `trip_duration` should not be used as a predictor for `tip_amount.` 
 

### CRISP-DM: Data Preparation

#### 2.1 Data preprocessing

To handle missing data in the `ehail_fee`, the entire column contains missing values NA. We can drop entire `ehail_fee` column since deleting it will not introduce bias.

Since we no longer need z2 and z3 columns that we created above, we also deleted these columns.

```{r}
# Delete the column of ehail_fee, z2, z3 and summary the result
tripdata_df$ehail_fee <- NULL
tripdata_df$z2 <- NULL 
tripdata_df$z3 <- NULL 

summary(tripdata_df)
```

Based on the previous data exploration, we first check for missing values before proceeding to outlier handling and further data cleaning.
```{r}
# Check all missing values in each column
missing_values <- colSums(is.na(tripdata_df))

print(missing_values)
```

There are 5 missing values in the `total_amount` column, 5 missing values in `fare_amount` column, 3 missing values in `trip_type` column.

We extract the tip_amount column as a separate variable for convenience in further analysis. 

```{r}
# Extract data that contains tip_amount data
tripdata_tip_amount <- tripdata_df$tip_amount
```

After checking the missing values, we find that `fare_amount`, `total_amount`, and `trip_type` still contain some missing entries. We remove all rows with missing values in these columns.
For `payment_type`, we only keep entries labeled as 1 (credit card) and 2 (cash payment), as these are the most relevant to tip behavior.

```{r}
# Only keep the non-missing values
tripdata_df_cleaned <- tripdata_df %>%
  filter(!is.na(trip_type)) %>%
  filter(!is.na(total_amount)) %>%
  filter(!is.na(fare_amount)) %>%
  
  # Keep only credit card type contribute to the tip amount
  filter(payment_type == 1) 
```

Since a trip cannot end before it starts, so we filter out rows where the dropoff time is earlier than the pickup time.
And then we remove rows where trip duration or passenger count is 0 or negative, which are not realistic values.
In last step in this part, we filter all positive values in those features because all monetary-related columns should be zero or positive.

The `mta_tax` column typically has a fixed value of 0.5 which is automatically triggered based on the metered rate in use. As a result, this column should only contain positive values, and any entries with a value of 0 are likely incorrect and should be filtered out.

```{r}
# Remove records where dropoff time is earlier than pickup time
tripdata_df_cleaned <- tripdata_df %>%
  filter(lpep_pickup_datetime < lpep_dropoff_datetime)

# Keep only trips with valid duration and passenger count
tripdata_df_cleaned <- tripdata_df %>%
  filter(trip_duration >0) %>%
  filter(passenger_count >0)

# Remove negative or invalid values
tripdata_df_cleaned <- tripdata_df %>%
  filter(mta_tax > 0) %>%
  filter(improvement_surcharge >= 0) %>%
  filter(extra >= 0) %>%
  filter(tip_amount >= 0) %>%
  filter(fare_amount >= 0) %>%
  filter(total_amount >= 0) %>%
  
  # Keep only valid RatecodeID values-1 to 6
  filter(as.numeric(RatecodeID) >= 1 & as.numeric(RatecodeID) <= 6)
```

In this step, we perform z-score outlier detection on all numeric columns, any value with a z-score greater than 3 (in absolute value) is considered an outlier and removed. Outliers are values that lie more than 3 standard deviations away from the mean

```{r}
# Remove outliers using z-score (|z| > 3) in all numeric columns
remove_outliers <- function(data) {
  numeric_columns <- names(data)[sapply(data, is.numeric)]
  
  # Loop through each numeric column
  for (col in numeric_columns) {
    mean_val <- mean(data[[col]], na.rm = TRUE) # mean
    sd_val <- sd(data[[col]], na.rm = TRUE) # standard deviation
    z <- abs((data[[col]] - mean_val) / sd_val) # z-score for each value
    
    # Removes extreme outliers beyond 3 standard deviations
  data <- data[z <= 3 | is.na(z), ]
  }
  
  return(data)
}

# Apply outlier removal
tripdata_df_cleaned <- remove_outliers(tripdata_df_cleaned)
```

#### 2.2 Normalize data

In this step, we normalize all numeric features except for the target variable `tip_amount`, as normalization should only be applied to predictors. 
The resulting dataset will be used min-max scaling. This transformation brings all values into the range [0, 1], which helps ensure that variables are on a comparable scale.

```{r}
#Select numeric and remove target variable 
tripdata_numeric_knn <- tripdata_df_cleaned %>% select_if(is.numeric) %>% select(-tip_amount) 

# Create a pre-processing model using min-max normalization
preproc_minmax <- preProcess(tripdata_numeric_knn, method=c("range"))

# Apply the normalization model to the numeric data
norm_minmax <- predict(preproc_minmax, tripdata_numeric_knn)

# Check the result
summary(norm_minmax)
```

We have to make sure that we exclude the target variable (tip_amount) when normalizing the data and keep this intact as the reference. All numeric variables are now scaled to a range between 0 and 1. 

#### 2.3 Encode the data 

In the data encoding step, to prepare the dataset for modeling, we encode all categorical features to create dummy variables. Nominal variables are one-hot encoded using the dummyVars() function from the caret package. The binary variable such as `store_and_fwd_flag`, `VendorID`, and `trip_type` are label-encoded. The resulting dataset contains only numeric features.

```{r}
# Create dummy variables
encode_variables <- function(df) {
  # One-hot encode categorical variables using dummyVars()
  vars_to_dummy <- c("payment_type", "RatecodeID")
  dummy_model <- dummyVars(~ ., data = df[, vars_to_dummy])
  dummy_df <- as.data.frame(predict(dummy_model, newdata = df))
  
  # Encode binary variable store_and_fwd_flag 
  df$store_and_fwd_flag <- ifelse(df$store_and_fwd_flag == "Y", 1, 0)
  df$VendorID <- ifelse(df$VendorID == "1", 1, 0)  
  df$trip_type <- ifelse(df$trip_type == "1", 1, 0)

  # Remove original nominal vars
  df_encoded <- df %>%
    select(-all_of(vars_to_dummy))  
  
  # Combine cleaned df with dummy variables
  final_df <- bind_cols(df_encoded, dummy_df)
  return(final_df)
}

# Apply encoding to the cleaned data 
tripdata_df_encoded <- encode_variables(tripdata_df_cleaned)

# Double check the tripdata_df_encoded
str(tripdata_df_encoded)
```

We created the encode_variables function to encode all variables. To double check, we used str() and saw that the tripdata_df_encoded has all categorical variables encoded to become dummy variables with values of 0 and 1. 

Next, we replaced the original numeric columns in `tripdata_df_encoded` with their normalized counterparts from `norm_minmax`
We used intersect() to ensure only the common numeric columns are replaced, preserving any non-numeric or newly encoded features. The tip_amount target variable is still kept intact. 

```{r}
# List of columns to replace from norm_minmax
columns_to_replace <- intersect(names(norm_minmax), names(tripdata_df_encoded))

# Replace values in tripdata_df_encoded with normalized values from norm_minmax
tripdata_df_encoded[columns_to_replace] <- norm_minmax[columns_to_replace]

# Make sure that the tripdata_df_encoded has 
str(tripdata_df_encoded)
```

The columns in norm_minmax have been added into tripdata_df_encoded. The data is ready for training and testing in the next step. The tripdata_df_encoded has 840083 rows and 29 columns. 

#### 2.4 Prepare the data for modeling

We splitted the dataset into training (80%) and testing (20%) sets. 
We used stratified sampling based on the target variable `tip_amount` to ensure similar distribution in both train and test sets.

```{r}
# Tripdata_df_encoded will be the one to use for the splitting 

set.seed(123) # for reproducibility
train_index <- createDataPartition(tripdata_df_encoded$tip_amount, p = 0.8, list = FALSE)

# Set the data in training and testing sets
data_train <- tripdata_df_encoded[train_index, ]
data_test <- tripdata_df_encoded[-train_index, ]

# Check the dimensions of the data_train and data_test 
dim(data_train)
dim(data_test)
```

We set random seed for reproducibility. The `data_train` has 80% of the total data. It has 672068 rows and 29 columns. The `data_test` has 20% of the total data. It has 168015 rows and 29 columns. 

The training set (80%) is used to "teach" the KNN model—this is where it stores all the examples it will later use to find the k-nearest neighbors when making predictions. The testing set (20%) is used to evaluate the model's performance on unseen data, simulating how it would perform in a real-world setting. We chose a 80-20 split offers a good balance: there's enough data to train a robust model, while still retaining enough data to reliably test and assess performance. Having a solid 80% training pool gives the model more neighbors to choose from, improving accuracy.


##

## CRISP-DM: Modeling

In this step we will develop the k-nn regression model. Create a function with the following name and arguments: knn.predict(data_train, data_test, k);

data_train represents the observations in the training set,

data_test represents the observations from the test set, and

k is the selected value of k (i.e. the number of neighbors).

```{r}
# Create function knn_predict
knn_predict <- function(data_train, data_test, k) {
  # Extract predictors (numeric columns except tip_amount)
  train_X <- data_train %>% select(where(is.numeric)) %>% select(-tip_amount)
  test_X <- data_test %>% select(where(is.numeric)) %>% select(-tip_amount)
  
  # Extract target variable
  train_y <- data_train$tip_amount
  test_y <- data_test$tip_amount
  
  # Use knn.reg to predict
  knn_output <- knn.reg(train = train_X, test = test_X, y = train_y, k = k)
  
  # Extract predictions
  predictions <- knn_output$pred
  
  # Calculate Mean Squared Error
  mse <- mean((predictions - test_y)^2, na.rm = TRUE)
  
  return(mse)
}

# Test the function with k = 5
k <- 5
mse <- knn_predict(data_train, data_test, k)
print(paste("k=", k, "Mean Squared Error (MSE) =", mse)) 
```

The function knn_predict() takes three arguments: data_train, data_test, and k. It first selects all numeric predictor columns from the dataset, excluding tip_amount, which is the target variable. This ensures that only features are used in the k-NN model. These features contain key features such as fare_amount, extra, total_amount which are known to be significant predictor variables and influence the tip amount. Some other dummy variables like VendorID, RatecodeID and payment_type are also present in the model. 

Then, the function extracts the target variable tip_amount from both the training and testing sets. The k-NN regression is performed using the knn.reg() function from the FNN package, where we specify the training data (train), test data (test), target values (y), and the number of neighbors (k).

The model returns predicted tip amounts for the test set, which are stored in knn_output$pred. The Mean Squared Error (MSE) is then calculated between the predicted and actual tip_amount values from the test set using the mean() function. It is the squared differences between the predicted values and the actual tip amounts in the test set. 

We set the number of neighbors k = 5 and get the Mean Squared Error = 0.100320867946314. Since this is a regression problem, the closer the MSE is to 0, the better our model's predictions match the actual values. An MSE of 0.1003 suggests that, on average, the squared prediction error is around 10 cents. This is a relatively low error, which means the model is performing quite well in predicting tips based on the selected features. 

Considering that tip amounts are often difficult to predict in reality, this level of accuracy is quite good. Tipping behavior is influenced not only by fare-related data, but also by subjective factors like the passenger’s satisfaction, driver attitude, comfort during the ride, or even mood and cultural habits—all of which are difficult to be captured in the dataset. Given these limitations, achieving an MSE this low suggests that the model is doing a solid job based on the available features. This k-NN regression model demonstrates a strong prediction power, oﬀering a reliable foundation for understanding and estimating taxi tip amounts.




## CRISP-DM: Evaluation

Determine the best value of k and visualize the MSE. This step requires selecting different values of k and evaluating which produced the lowest MSE. 

```{r}
# Define a range of k values 
k_values <- c(1, 3, 5, 11, 15, 25, 31, 35, 50, 75, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)  # random 20 numbers
```

We picked 20 representative k values, ranging from very small ones like 1, 3, and 5 to much larger ones like 500 and 1000, to see how different k sizes affect the model’s prediction performance in terms of MSE. Smaller k values make the model focus more on nearby data points — they're more responsive but can easily overfit. Larger k values make the predictions smoother, which might sacrifice some accuracy but help reduce variance. By comparing different k values, we’re trying to find one that strikes a good balance between bias and variance.
 
In addition, we also considered the size of the training dataset when selecting K. A common rule of thumb is to set K = roughly the square root of the number of observations samples. In our case, the training set contains 672,068 records, and the square root is approximately 820. So we included some larger K values, such as 800, 900, and 1000, to test how the model performs in that range. We also included a few very small K values, like 1, 3, and 5, so that we could cover both ends of the scale. This approach helps us better understand how the model behaves with different numbers of neighbors, and ultimately choose a more suitable K value.
  
```{r}
# Define a function to test multiple k values and return their MSEs
test_multiple_k <- function(train_data, test_data, k_values) {
  mse_results <- numeric(length(k_values))
  
  for (i in seq_along(k_values)) {
    mse_results[i] <- knn_predict(train_data, test_data, k_values[i])
  }
  
  return(data.frame(k = k_values, mse = mse_results))
}

# Run the test_multiple_k() function on the full training and test sets. This will calculate the MSE for each k value in the list
test_multiple_k_results <- test_multiple_k(data_train, data_test, k_values)

# Print the result table showing each k and its corresponding MSE
#print(test_multiple_k_results)
```

We created a function called test_multiple_k() to evaluate the performance of the k-NN regression model for 20 k values by calling the knn_predict() function repeatedly in a loop. It calculates the Mean Squared Error (MSE) for each value of k and returns a dataframe containing the results.

```{r}
# Find the k value that gives the lowest MSE
best_k <- test_multiple_k_results$k[which.min(test_multiple_k_results$mse)]
print(best_k)

# Get the lowest MSE value from the results
lowest_mse <- min(test_multiple_k_results$mse)
print(lowest_mse)

# Create a line chart and plot
ggplot(test_multiple_k_results, aes(x = k, y = mse)) +
  geom_line() +
  geom_point() +
  labs(title = "KNN Regression for multiple k",
       subtitle = "Test 20 k values and evaluate which produced the lowest MSE",
       caption = "Find the most suitable k value",
       x = "K Values (k)",
       y = "Mean Squared Error") +
  theme_minimal() + 
  theme(plot.caption = element_text(hjust=0, face = "italic"))
```

Based on the result table, we can see that the MSE is lowest when k = 3 (0.09971859), so this appears to be the best option for now. That said, the MSE values for k between 1 and 5 are all very close — they range from 0.1 to 0.13, which is a pretty small difference. From the line chart, we can also see that as k increases, the MSE keeps going up, meaning the model becomes too simple and starts underfitting. So larger k values clearly don’t work well here. Within the smaller k range, such as k = 3 and k = 5, the model performs similarly. Compared to them, k = 1 also gives a low MSE, but it's more sensitive to noise and less stable, so we don’t recommend using it. Once k goes beyond 5, the MSE increases more significantly and the model performance drops. Overall, k = 3 gives a good balance between bias and variance. Based on this analysis, it's the most suitable choice.

The dataset is large, yet the k-NN regression model performs best when using a very small number of neighbors (k = 3). This suggests that tipping behavior is complex and highly context-specific, likely influenced by nuanced characteristics of each individual trip. A low k can capture these fine-grained local patterns, while a higher k incorporates more distant (and potentially unrelated) data points, which adds noise rather than useful signal—leading to a higher Mean Squared Error (MSE).

The larger k is, the larger MSE becomes, the model's performance decreases. This trend reinforces the idea that tipping is not easily generalizable across broad trends and instead varies significantly across specific circumstances, such as ride quality, customer satisfaction, or driver behavior—factors that may not be fully captured in the dataset.

Because of a relatively small MSE, the k-NN regression model showed good performance on the current dataset. When k = 3, it achieved the lowest validation MSE (0.09971859), indicating that the model has a certain level of predictive ability under the current data split. Given that k-NN is simple in structure and does not require complex training, it’s a good fit for projects like this one, where the dataset is moderately sized and the feature structure is relatively straightforward. For the task of predicting tip amounts, we believe this model is suitable for short-term forecasting or as an initial analysis tool. That said, the model does have some limitations. KNN is sensitive to the distribution of the data, and when the training set is small or contains noise, it can easily overfit. It also relies heavily on input features—if key variables such as time of travel or weather are missing, the model may lack stability when predicting tips for future trips. To sum up, we think this model is appropriate for short-term tip prediction tasks, especially in exploratory analysis or early-stage modeling. However, if the goal involves long-term forecasting or requires higher levels of stability and performance, more efficient and robust modeling approaches would be needed.

To enhance the model’s performance, we believe feature engineering could play a key role. By creating new and more informative predictors, we can better capture the subtle and complex factors that influence tipping behavior—such as ride duration, time of day, day of the week, traffic conditions, or location-based patterns. Additionally, exploring more advanced machine learning algorithms—such as decision trees, random forests, or gradient boosting—may help model these complexities more effectively than k-NN, especially when interactions between variables or non-linear patterns are present.











#### Q5



```{r}
tripdata_daily <- tripdata_df %>%
  mutate(pickup_date = as.Date(lpep_pickup_datetime)) %>%
  group_by(pickup_date) %>%
  summarise(daily_trips = n())

daily_trip_plot <- ggplot(tripdata_daily, aes(x = pickup_date, y = daily_trips)) +
  geom_line(color = "steelblue", size = 0.7) +
  geom_smooth(method = "loess", se = FALSE, color = "darkred", linetype = "dashed") +
  labs(
    title = "Daily NYC Green Taxi Trip Counts (2018)",
    subtitle = "Aggregated trips per day to reveal trends and seasonality",
    x = "Date",
    y = "Number of Trips",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))
  
print(daily_trip_plot)

```
#Daily NYC Green Taxi Trip Counts(2018)
Blue Line: Represents the actual daily trip counts for 2018.
Brown Dashed Line: Represents a trend or seasonal pattern, possibly a fitted curve or a moving average.
The trend/seasonality line (brown dashed) is not accurate and should be ignored due to negative and irrelevant values.

heatmap; Trip Activity by Hour and Weekday

```{r}
# extracing hr: day and weekday from pickup datetime
tripdata_time <- tripdata_df %>%
  mutate(
    hour = lubridate::hour(lpep_pickup_datetime),
    weekday = lubridate::wday(lpep_pickup_datetime, label = TRUE, abbr = FALSE)
  )

hourly_activity <- tripdata_time %>%
  group_by(weekday, hour) %>%
  summarise(trips = n(), .groups = "drop")

# heatmap
heatmap_activity <- ggplot(hourly_activity, aes(x = hour, y = weekday, fill = trips)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma") +
  labs(
    title = "NYC Green Taxi Activity by Hour and Weekday",
    subtitle = "Heatmap of trip counts across hours of the day and days of the week",
    x = "Hour of Day (0-23)",
    y = "Day of Week",
    fill = "Trips Count",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))
  
print(heatmap_activity)

```4
#Heatmap
NYC Green Taxi demand is highest on Friday evenings and weekday late afternoons.
Demand is lowest in the early morning hours and on Sundays.
The heatmap visually highlights these patterns, helping taxi operators and city planners understand when and where demand is greatest.
The busiest times for green taxis are during the late afternoon and early evening, especially on Fridays, likely due to people commuting home or going out.


Trip Distance, Duration, and Tip Amount


```{r}
# here we are using the cleaened data
trip_scatter <- ggplot(tripdata_df, aes(x = trip_distance, y = trip_duration, color = tip_amount)) +
  geom_point(alpha = 0.4) +
  scale_color_viridis_c(option = "inferno", name = "Tip Amount") +
  labs(
    title = "Trip Distance vs. Duration Colored by Tip Amount",
    subtitle = "Exploring how trip length and time relate to tipping behavior",
    x = "Trip Distance (miles)",
    y = "Trip Duration (minutes)",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))
  
print(trip_scatter)

```
# Trip Distance vs Duration Colored by Tip Amount
1. Most Trips Are Short and Quick:
The majority of points are clustered in the lower left corner, indicating that most taxi trips are both short in distance (under 10 miles) and short in duration (under 100 minutes).
2. Longer Trips Are Rare:
There are a few trips with much longer distances (up to 100+ miles) and/or durations (up to 3000+ minutes), but these are outliers.
3. Tip Amounts:
Most points are dark, meaning most tips are small.
A few points are lighter (yellow/orange), indicating higher tips, but these are rare and scattered.
4. Possible Data Anomalies:
Some trips have extremely long durations (over 1000 minutes, or more than 16 hours), which is unusual for a taxi ride and may indicate data entry errors or special cases.


Fare Amount vs. Tip Amount by Rate Code

```{r}
trip_fare_tip <- tripdata_df_cleaned %>%
  filter(fare_amount > 0, tip_amount >= 0)

# Create the scatter plot with facets for each RatecodeID
fare_tip_plot <- ggplot(trip_fare_tip, aes(x = fare_amount, y = tip_amount)) +
  geom_point(alpha = 0.3, color = "darkblue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  facet_wrap(~ RatecodeID) +
  labs(
    title = "Fare Amount vs. Tip Amount by Rate Code",
    subtitle = "Scatter plots with regression lines for each Rate Code category",
    x = "Fare Amount (USD)",
    y = "Tip Amount (USD)",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))
  
print(fare_tip_plot)

```

# Fare Amount vs. Tip Amount by Rate Code
Scatter plots with regression lines for each Rate Code category
1. Rate Code 1 (Top Left)
Most Data: The majority of trips fall under this code.
Trend: As fare increases, tip amount generally increases, but not perfectly linearly. Most tips are under $50, even for high fares.
Outliers: Some trips have very high tips, but these are rare.

2. Rate Code 2 (Top Middle)
Few Data Points: Very few trips.
Trend: No clear pattern due to limited data.

3. Rate Code 4 (Top Right)
Moderate Data: Some trips, but fewer than Rate Code 1.
Trend: Positive relationship between fare and tip, similar to Rate Code 1, but with less spread.

4. Rate Code 5 (Bottom Left)
Few Data Points: Limited data.
Trend: Slight positive relationship, but not much variation.

5. Rate Code 6 (Bottom Middle)
Almost No Data: Only one or two points, so no visible trend.



Box Plot of Tip Percentage by Hour of Day

```{r}
trip_tip <- tripdata_df_cleaned %>%
  mutate(
    tip_percentage = 100 * tip_amount / fare_amount,
    pickup_hour = lubridate::hour(lpep_pickup_datetime)
  ) %>%
  filter(is.finite(tip_percentage))  # Remove any non-finite cases

# Create box plot of tip percentage by pickup hour
tip_percentage_plot <- ggplot(trip_tip, aes(x = factor(pickup_hour), y = tip_percentage)) +
  geom_boxplot(fill = "skyblue", outlier.alpha = 0.3) +
  labs(
    title = "Tip Percentage by Hour of Day",
    subtitle = "Variation in tip percentage (tip relative to fare) across different hours",
    x = "Hour of Day (0-23)",
    y = "Tip Percentage (%)",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))
  
print(tip_percentage_plot)

```
#Tip Percentage by Hour of Day
- Each dot represents a single taxi trip, showing the tip percentage for that trip at a given hour.
- There are some trips with extremely high tip percentages (over 1000%, and even up to 8000%). These are likely due to data entry errors, unusually low fares with standard tips, or other anomalies.
- The pattern of tip percentages is fairly consistent across all hours of the day, with no obvious hour where tips are dramatically higher or lower on average.
- Outliers appear at almost every hour, but the bulk of the data remains tightly packed at lower percentages.

Average Tip Percentage by Hour of Day

```{r}
library(dplyr)
library(ggplot2)
library(lubridate)

trip_tip_hour <- tripdata_df %>%
  mutate(
    tip_percentage = ifelse(fare_amount > 0, 100 * tip_amount / fare_amount, NA),
    pickup_hour = hour(lpep_pickup_datetime)
  ) %>%
  filter(!is.na(tip_percentage)) %>%
  group_by(pickup_hour) %>%
  summarise(
    avg_tip_percentage = mean(tip_percentage, na.rm = TRUE), 
    n_trips = n()
  )

# Create a line plot for the average tip percentage by pickup hour
avg_tip_line <- ggplot(trip_tip_hour, aes(x = pickup_hour, y = avg_tip_percentage)) +
  geom_line(color = "blue", size = 1.2) +
  geom_point(color = "blue", size = 2) +
  labs(
    title = "Average Tip Percentage by Hour of Day",
    subtitle = "How tipping behavior (as a percentage of fare) varies over the day",
    x = "Hour of Day",
    y = "Average Tip Percentage (%)",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))

print(avg_tip_line)

```


1. The average tip percentage hovers around 10% for most hours of the day.
2. There is a noticeable dip in the early morning (around 5 AM), where the average tip percentage drops below 8%.
3. Immediately after this dip, there is a sharp spike at 6 AM, where the average tip percentage jumps to nearly 18%—the highest point on the graph.
4. After this spike, the tip percentage quickly returns to the 9–10% range and remains relatively stable, with a slight increase in the late afternoon and evening.


```{r}
library(dplyr)
library(ggplot2)

payment_trip_counts <- tripdata_df %>%
  filter(!is.na(trip_type), !is.na(payment_type)) %>%
  group_by(payment_type, trip_type) %>%
  summarise(count = n(), .groups = "drop")

# Create a stacked bar chart showing counts per payment type split by trip type
payment_trip_plot <- ggplot(payment_trip_counts, aes(x = payment_type, y = count, fill = trip_type)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    title = "Stacked Bar Chart of Payment Type by Trip Type",
    subtitle = "How payment methods distribute across different trip types",
    x = "Payment Type",
    y = "Number of Trips",
    fill = "Trip Type",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))

print(payment_trip_plot)

```
# Payment Type
Credit Card (1): The most common payment method, with nearly 500,000 trips.
Cash (2): The second most common, with just under 400,000 trips.
The graph shows that nearly all NYC Green Taxi trips in 2018 were paid for by credit card or cash, with credit cards being slightly more common. The vast majority of these trips are of Trip Type 1, while Trip Type 2 is rare. Other payment types are almost never used.



```{r}
library(dplyr)
library(ggplot2)
library(lubridate)

trip_daily_revenue <- tripdata_df %>%
  mutate(
    pickup_date = as.Date(lpep_pickup_datetime),
    total_revenue = fare_amount + tip_amount
  ) %>%
  group_by(pickup_date) %>%
  summarise(
    daily_revenue = sum(total_revenue, na.rm = TRUE),
    avg_tip = mean(tip_amount, na.rm = TRUE)
  )

# Find scaling factor to use the secondary axis (to match range of daily_revenue)
scale_factor <- max(trip_daily_revenue$daily_revenue, na.rm = TRUE) / max(trip_daily_revenue$avg_tip, na.rm = TRUE)

# Create the plot: bars for daily revenue and a line for average tip amount
revenue_plot <- ggplot(trip_daily_revenue, aes(x = pickup_date)) +
  geom_bar(aes(y = daily_revenue), stat = "identity", fill = "lightblue", alpha = 0.6) +
  geom_line(aes(y = avg_tip * scale_factor), color = "darkred", size = 1.2) +
  scale_y_continuous(
    name = "Daily Revenue (USD)",
    sec.axis = sec_axis(~ . / scale_factor, name = "Average Tip Amount (USD)")
  ) +
  labs(
    title = "Daily Total Revenue and Average Tip Amount",
    subtitle = "Bar chart represents total revenue per day; red line shows average tip amount",
    x = "Date",
    caption = "Data source: NYC Green Taxi Trip Records (2018)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(face = "italic", size = 10))

print(revenue_plot)

```

#Daily Total Revenue and Average Tip Amount

2009–2012: There is a sharp increase in daily revenue, peaking around 2011.

2012–2018: After the peak, daily revenue gradually declines.

2018: There is a sudden spike in both revenue and average tip amount, but this may be due to a data artifact or a change in data collection.
The 2018 spike should be interpreted carefully, as it may not reflect a real-world trend but rather a data anomaly.
The sudden jump in both revenue and average tip amount is unusual and may indicate a change in data reporting, a one-time event, or an error in the dataset.
